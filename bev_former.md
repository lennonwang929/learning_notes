

# bev_former

- 有6个编码层，主体框架与传统transformer一样
- 
## Transformer
QKV
q问的是在不在前面，k是回答，它俩相乘计算的是相关性，用相关性乘以v，得到一个delta变化的向量，基础向量加这个向量就会得到一个新的向量，这个向量会丰富原词语，在前面丰富，因为q问的是在不在前面，但是这个在不在前面是怎么体现的，应该是在回传更新的时候体现，或者是在不在前面这句话有一个向量encoding?那又是怎么体现的，又或者是在q*k后将后面的都设置为0？
q和k的维度会比较低，所以乘以的w维度与v的不同？q和k相当于作了降维，v没有降维，得到的就是一个与其他embedding在同等维度的向量，虽然是delta，但它要用于加法运算，也不能降维

“汇聚”通常指的是将多个向量或信息整合成一个单一的表示

k、v相当于已知的x_ i，y_i，q是未知的x，用已知的去求f(x)，
**Transformer 主要用于分类任务和生成任务，而不是回归任务**
### **Transformer 架构如何适应不同任务**

Transformer 的核心架构，包括**多头自注意力机制**和**前馈神经网络**，本质上是通用的。根据不同的任务，它的输出层和损失函数会有所不同：

-   **分类任务**：
    
    -   输出层通常是一个全连接层，后接 Softmax 激活函数，用于将输出转化为类别概率分布。
    -   损失函数通常使用交叉熵损失函数（Cross-Entropy Loss），用来衡量预测的类别分布与真实类别之间的差异。
-   **回归任务**：
    
    -   输出层是一个线性层，不使用激活函数，直接输出连续值。
    -   损失函数通常是均方误差（Mean Squared Error, MSE）或均方根误差（Root Mean Squared Error, RMSE），用来衡量预测值与真实值之间的差异。
 
 回归算法和近似算法虽然都是计算和数据分析中的工具，但它们属于不同的概念范畴，尽管在某些应用场景中有交叉和联系。让我们深入理解它们各自的定义以及它们之间的关系。

### 1. **回归算法**
   **回归算法**属于**监督学习**中的一种，用来建模变量之间的关系，特别是预测连续值输出。它的目标是找到一个函数，能够最好地拟合给定的输入和输出数据。

   - **回归的目标**：通过观察输入特征（自变量）和输出（因变量）之间的关系，找到最能描述这种关系的数学模型。常见的回归模型有线性回归、逻辑回归（虽然用于分类，但仍称为回归）、多项式回归等。
   
   - **典型任务**：预测房价、股票价格、温度、销售额等连续值。
   
   - **数学形式**：回归模型通常会假设一个形式，例如：
     \[
     y = f(x) + \epsilon
     \]
     其中 \(f(x)\) 是一个函数（模型），\(\epsilon\) 是误差项。目标是最小化模型预测值和真实值之间的误差。

### 2. **近似算法**
   **近似算法**通常属于**优化**或**数值分析**领域，目的是在一些复杂或难以求解的问题中找到一个可接受的近似解，而不一定要求最优解。

   - **近似的目标**：近似算法的设计动机是解决那些无法用精确算法快速解决的问题，特别是在大规模或复杂问题中。近似算法通过牺牲精度换取计算效率，从而在有限的时间内得到一个近似解。
   
   - **典型应用**：近似算法常用于图论、组合优化、数值分析等问题中，如旅行商问题（TSP）、背包问题等，它们通过一些启发式方法（如贪心算法、模拟退火、遗传算法等）寻找近似解。
   
   - **特点**：近似算法的目标不是找到问题的精确解，而是得到接近最优解的结果。在某些情况下，它会有一定的误差边界，确保近似解与最优解的差距在一定范围内。

### 3. **回归算法和近似算法的区别**
   - **目标不同**：
     - 回归算法的主要目的是通过数据拟合找到变量间的函数关系，并预测新的数据点的连续值。
     - 近似算法的目标是通过特定的启发式或优化策略，快速找到一个复杂问题的可接受解，而不追求精确解。
   
   - **应用场景不同**：
     - 回归算法主要用于预测、分析和建模，例如预测股价或温度，这类问题涉及连续的数值输出。
     - 近似算法主要用于组合优化、图论或其他复杂的求解问题，例如优化物流路径、寻找最大独立集等。
   
   - **方法不同**：
     - 回归算法基于统计学和机器学习原理，常用的方法包括梯度下降、最小二乘法等。
     - 近似算法则使用启发式或随机搜索的方法，如模拟退火、粒子群优化等，以降低计算复杂度。

###  **回归与近似的关系**
   虽然回归算法和近似算法属于不同的领域，但它们在某些方面有交集，尤其是在处理**复杂非线性问题**时。

   - **回归中的近似**：
     在回归问题中，当数据关系非常复杂且无法用简单的线性模型精确表示时，模型可以通过近似的方法来处理。比如，使用高阶多项式回归或神经网络时，回归算法本质上是在逼近复杂的函数关系。这时，回归模型可以看作是对真实关系的一个**近似**。
   
   - **优化中的回归**：
     有时，回归问题可以被视为一个优化问题。模型训练过程中，我们使用优化算法（如梯度下降）来最小化损失函数。对于一些复杂的回归问题，可能需要借助一些启发式的优化策略，从而获得接近最优的模型参数。

   - **回归和近似的共同特性**：
     二者都可能涉及“逼近”或“拟合”复杂的目标。例如，回归模型试图通过训练数据来拟合一个近似的函数，而近似算法通过计算找到复杂优化问题的接近解。因此，从广义上讲，回归算法在预测复杂数据关系时，可以被视为一种“函数近似”的方法。

### 5. **举例说明**
   - **回归问题的例子**：
     假设你想预测未来一段时间的股票价格。你收集了股票的历史价格、成交量、市场趋势等数据，然后使用线性回归、决策树或神经网络来建立一个模型，该模型拟合了输入特征与股票价格之间的关系。这个模型可以近似地表示股票价格的变化趋势，但它并不是一个精确的函数。

   - **近似算法的例子**：
     假设你需要解决一个大型组合优化问题，比如旅行商问题（TSP），寻找访问一组城市的最短路径。使用精确算法求解可能非常耗时，因此你使用遗传算法（近似算法）来快速得到一个接近最优的解。这是通过启发式策略实现的，但不会给出真正的最优解。

### 总结
- **回归算法**主要用于拟合和预测变量之间的关系，目标是预测连续值，是一种监督学习方法。
- **近似算法**用于复杂的优化或求解问题，通过启发式方法找到接近最优解的结果，属于优化领域。
- 虽然它们属于不同领域，但在一些场景中存在交叉，尤其是回归算法在建模复杂非线性关系时可以被看作是一种函数近似方法。


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTU5MDczMDE1NSwtMjA0ODQyNzA3LDE2Nj
A5NzQxNzIsLTExOTY1OTMzMjddfQ==
-->