

# bev_former

- 有6个编码层，主体框架与传统transformer一样
- 
## Transformer
QKV
q问的是在不在前面，k是回答，它俩相乘计算的是相关性，用相关性乘以v，得到一个delta变化的向量，基础向量加这个向量就会得到一个新的向量，这个向量会丰富原词语，在前面丰富，因为q问的是在不在前面，但是这个在不在前面是怎么体现的，应该是在回传更新的时候体现，或者是在不在前面这句话有一个向量encoding?那又是怎么体现的，又或者是在q*k后将后面的都设置为0？
q和k的维度会比较低，所以乘以的w维度与v的不同？q和k相当于作了降维，v没有降维，得到的就是一个与其他embedding在同等维度的向量，虽然是delta，但它要用于加法运算，也不能降维
## Transformer与子空间聚类
子空间聚类是找到一组来自同一个子空间内的其他向量来表示这个向量，X=XZ，得到的这个表示矩阵Z中的元素，可以作为系数将其他向量线性组合为这个向量，而自注意力机制用三个不同的权重矩阵分别对原特征进行处理，目的是获得几个不同视角的特征，但来源一致，不同于X=XZ中，从头到尾都用的是X，与某个向量相关性较大的一组其他向量，这些向量可以通过线性组合表示这个向量，通过注意力权重对这些向量进行线性组合会生成一个新向量，而这个新向量与原来的这个向量会很相似，但又有细微的差别，从而通过这种方式生成新向量
“汇聚”通常指的是将多个向量或信息整合成一个单一的表示
### **为什么不直接使用输入序列作为 K 和 V？**

在 Transformer 中，我们不直接将输入向量作为 K 和 V，而是通过线性变换生成新的向量，这有几个重要的原因：

-   **特征提取**：通过不同的权重矩阵生成 Q、K、V，可以提取出输入序列的不同视角或特征。K 和 V 通过权重矩阵的变换可以捕捉到输入序列的不同方面的信息，而不是简单地使用原始输入。
    
-   **控制维度**：Q、K、V 通过线性变换可以对维度进行调整。比如，在多头注意力机制中，Q、K、V 的维度通常会被缩小，这样可以并行计算多个注意力头。
    
-   **增强灵活性**：线性变换提供了更大的模型灵活性和学习能力。在不同任务中，Q、K、V 的权重矩阵可以学习到不同的特征表示，这使得模型具有更好的泛化能力。
k、v相当于已知的x_ i，y_i，q是未知的x，用已知的去求f(x)，
**Transformer 主要用于分类任务和生成任务，而不是回归任务**
一个通用的注意力汇聚（Attention Pooling）公式通常涉及对输入进行加权汇聚（加权平均），其中每个输入的权重由注意力机制动态计算得到。**注意力汇聚**的核心思想是：根据输入之间的相关性，为每个输入分配不同的权重，进而对输入特征进行加权汇聚，输出一个全局的表示。

### 注意力机制
注意力机制是一种通过计算输入之间相关性来动态聚焦于重要信息的技术。其核心思想是：在处理一组输入时，模型可以根据输入之间的相关性，为每个输入分配不同的权重，而不是简单地处理所有输入都同等重要。

### 通用的注意力汇聚公式

假设有一组输入特征 $\{x_1, x_2, ..., x_n\}$，这些输入特征通过注意力机制计算出相应的权重 $\{\alpha_1, \alpha_2, ..., \alpha_n\}$，注意力汇聚的输出可以表示为：
$$ \text{Attention Output} = \sum_{i=1}^{n} \alpha_i x_i$$
其中：
- $x_i$ 是第$i$ 个输入特征向量（如序列中的词向量或图像中的像素特征）。
- $\alpha_i$ 是第 i 个输入的注意力权重，反映了输入 $x_i$ 在当前任务中所占的重要性。
- $\sum_{i=1}^{n} \alpha_i = 1$，即所有的注意力权重总和为 1，这样可以保证得到的加权汇总是一个合适的比例。

### 具体步骤

1. **计算相似度**：
   首先，需要计算查询（Query, q）和键（Key, k_）之间的相似度。常用的方法是点积或者其他相似度度量方法。以点积为例：
   
  $$ \text{score}(q, k_i) = q \cdot k_i$$
   
   其中，q 是查询向量，$k_i$ 是第$i$个输入的键向量。

2. **注意力权重的计算**：
   对所有输入的相似度进行归一化，通常使用 Softmax 函数将相似度转化为概率分布：
   
  $$ \alpha_i = \frac{\exp(\text{score}(q, k_i))}{\sum_{j=1}^{n} \exp(\text{score}(q, k_j))}$$
   
   这样可以保证所有的权重 $\alpha_i$ 都是非负的，且总和为 1。

3. **加权汇聚**：
   最后，将输入特征按照权重 $\alpha_i$ 进行加权求和，得到汇聚后的全局表示：
   
   $$ \text{Attention Output} = \sum_{i=1}^{n} \alpha_i v_i$$
   
   其中，$v_i$ 是第 个输入的值（Value），表示输入的特征信息。通常，值向量 $v_i$和输入特征$x_i$ 是一样的，但它们可以是不同的特征表示。

### 公式的含义

1. **查询（Query）**：查询向量 q 代表当前任务的目标，通常是在自注意力（Self-Attention）中由输入序列的某个时间步生成，或者在跨注意力（Cross-Attention）中由其他输入（如解码器输入）生成。

2. **键（Key）**：键向量 $k_ i$代表输入特征$x_i$ 的一种特征表示，用于与查询进行相似度计算。它用于衡量输入特征与查询的相关性。

3. **值（Value）**：值向量 $v_i$ 是最终需要加权汇聚的输入特征信息，通常和键 $k_i$ 一起从输入特征 $x_i$ 中生成。

4. **权重（$\alpha_i$)**：注意力权重 $\alpha_i$ 是根据查询和键之间的相似度动态计算得到的，反映了输入$x_i$ 对当前查询的贡献大小。通过 Softmax 归一化，所有的权重加起来为 1。

5. **加权汇聚**：最终的注意力输出是对所有输入特征的加权汇聚，其中权重根据查询与输入的相关性动态分配。这种机制可以让模型根据任务的需要动态地选择最重要的信息，而忽略不相关的部分。

## 注意力评分函数
**注意力评分函数**（Attention Score Function）指的就是计算查询向量 $Q$ 和键向量 $K$ 之间相似度的方式。它在注意力机制中起着关键作用，通过对 $Q$ 和$K$ 的相似性进行评分，来确定每个值向量 $V$ 的重要性。

### 常见的注意力评分函数

1. **点积注意力（Dot-Product Attention）**：
   这是最常用的评分函数，计算 $K$ 的点积来衡量相似度：
   $$
   \text{score}(Q, K) = \frac{Q \cdot K^T}{\sqrt{d_k}}
   $$
   其中，$d_k$ 是键向量 $K$ 的维度，点积表示查询和键向量之间的相似度，使用 $\sqrt{d_k}$ 进行缩放是为了避免点积值过大，影响梯度稳定性。

2. **加性注意力（Additive Attention）**：
   加性注意力不是直接通过点积计算相似度，而是将 $Q$ 和 $K$ 拼接起来通过一个前馈神经网络来计算得分：
   $$
   \text{score}(Q, K) = \text{V}^T \cdot \text{tanh}(W_q Q + W_k K)
   $$
   这里 $W_q$ 和 $W_k$ 是可学习的权重矩阵，$\text{V}$ 是一个可学习的向量。这种方法主要用于早期的注意力机制，比如 Bahdanau Attention。

3. **双线性评分（Bilinear Attention）**：
   双线性评分函数通过一个可学习的矩阵 $W$ 连：
   $$
   \text{score}(Q, K) = Q^T W K
   $$
   这种方法比点积注意力更灵活，但计算开销更大。

### 注意力评分函数的作用

注意力评分函数的主要作用是计算 $$ $ $ 的相似度，这个相似度表示查询向量和每个键向量之间的相关性。相关性越大，生成的注意力权重就越高，从而在输出时赋予相应的值向量 $$ 更大的权重。

### 举个例子

假设有两个查询和键向量：

- $ Q = [1, 0.5] $
- $ K_1 = [1, 0.5] $
- $ K_2 = [0, 1] $

我们使用点积注意力计算相似度：
$$
\text{score}(Q, K_1) = \frac{Q \cdot K_1^T}{\sqrt{2}} = \frac{1 \times 1 + 0.5 \times 0.5}{\sqrt{2}} = \frac{1.25}{1.414} \approx 0.88
$$
$$
\text{score}(Q, K_2) = \frac{Q \cdot K_2^T}{\sqrt{2}} = \frac{1 \times 0 + 0.5 \times 1}{\sqrt{2}} = \frac{0.5}{1.414} \approx 0.35
$$

因此，$ Q $ 更加“关注”$ K_1 $，因为它们的相似度更高。

### Attention Pooling 的应用场景

- **自然语言处理（NLP）**：在序列到序列任务（如机器翻译）中，attention pooling 用来从输入序列中选择对当前任务最重要的词语。例如，在翻译句子时，解码器会根据注意力权重选择源语言句子中的最相关单词进行汇聚。

- **图像处理（CV）**：在图像分类任务中，attention pooling 可以用于从图像中选择关键的局部特征，将其汇聚为一个全局的图像表示。

- **时间序列预测**：在时间序列任务中，attention pooling 可以用来从过去的时间点中选择对未来预测最重要的特征，进行加权汇聚，生成用于预测的全局特征表示。

### 总结
通用的注意力汇聚公式如下：
$$\text{Attention Output} = \sum_{i=1}^{n} \alpha_i v_i, \quad \alpha_i = \frac{\exp(\text{score}(q, k_i))}{\sum_{j=1}^{n} \exp(\text{score}(q, k_j))}$$
它表示通过注意力机制计算输入特征的重要性权重，并将输入特征按照这些权重进行加权汇总，得到全局特征表示。
### **Transformer 架构如何适应不同任务**
注意力机制是一种通过计算输入之间相关性来动态聚焦于重要信息的技术。其核心思想是：在处理一组输入时，模型可以根据输入之间的相关性，为每个输入分配不同的权重，而不是简单地处理所有输入都同等重要。

Transformer 的核心架构，包括**多头自注意力机制**和**前馈神经网络**，本质上是通用的。根据不同的任务，它的输出层和损失函数会有所不同：

-   **分类任务**：
    
    -   输出层通常是一个全连接层，后接 Softmax 激活函数，用于将输出转化为类别概率分布。
    -   损失函数通常使用交叉熵损失函数（Cross-Entropy Loss），用来衡量预测的类别分布与真实类别之间的差异。
-   **回归任务**：
    
    -   输出层是一个线性层，不使用激活函数，直接输出连续值。
    -   损失函数通常是均方误差（Mean Squared Error, MSE）或均方根误差（Root Mean Squared Error, RMSE），用来衡量预测值与真实值之间的差异。
 
 回归算法和近似算法虽然都是计算和数据分析中的工具，但它们属于不同的概念范畴，尽管在某些应用场景中有交叉和联系。让我们深入理解它们各自的定义以及它们之间的关系。

### 1. **回归算法**
   **回归算法**属于**监督学习**中的一种，用来建模变量之间的关系，特别是预测连续值输出。它的目标是找到一个函数，能够最好地拟合给定的输入和输出数据。

   - **回归的目标**：通过观察输入特征（自变量）和输出（因变量）之间的关系，找到最能描述这种关系的数学模型。常见的回归模型有线性回归、逻辑回归（虽然用于分类，但仍称为回归）、多项式回归等。
   
   - **典型任务**：预测房价、股票价格、温度、销售额等连续值。
   
   - **数学形式**：回归模型通常会假设一个形式，例如：
    $y = f(x) + \epsilon$
     其中 f(x 是一个函数（模型），epsilon是误差项。目标是最小化模型预测值和真实值之间的误差。

### 2. **近似算法**
   **近似算法**通常属于**优化**或**数值分析**领域，目的是在一些复杂或难以求解的问题中找到一个可接受的近似解，而不一定要求最优解。

   - **近似的目标**：近似算法的设计动机是解决那些无法用精确算法快速解决的问题，特别是在大规模或复杂问题中。近似算法通过牺牲精度换取计算效率，从而在有限的时间内得到一个近似解。
   
   - **典型应用**：近似算法常用于图论、组合优化、数值分析等问题中，如旅行商问题（TSP）、背包问题等，它们通过一些启发式方法（如贪心算法、模拟退火、遗传算法等）寻找近似解。
   
   - **特点**：近似算法的目标不是找到问题的精确解，而是得到接近最优解的结果。在某些情况下，它会有一定的误差边界，确保近似解与最优解的差距在一定范围内。

### 3. **回归算法和近似算法的区别**
   - **目标不同**：
     - 回归算法的主要目的是通过数据拟合找到变量间的函数关系，并预测新的数据点的连续值。
     - 近似算法的目标是通过特定的启发式或优化策略，快速找到一个复杂问题的可接受解，而不追求精确解。
   
   - **应用场景不同**：
     - 回归算法主要用于预测、分析和建模，例如预测股价或温度，这类问题涉及连续的数值输出。
     - 近似算法主要用于组合优化、图论或其他复杂的求解问题，例如优化物流路径、寻找最大独立集等。
   
   - **方法不同**：
     - 回归算法基于统计学和机器学习原理，常用的方法包括梯度下降、最小二乘法等。
     - 近似算法则使用启发式或随机搜索的方法，如模拟退火、粒子群优化等，以降低计算复杂度。

###  **回归与近似的关系**
   虽然回归算法和近似算法属于不同的领域，但它们在某些方面有交集，尤其是在处理**复杂非线性问题**时。

   - **回归中的近似**：
     在回归问题中，当数据关系非常复杂且无法用简单的线性模型精确表示时，模型可以通过近似的方法来处理。比如，使用高阶多项式回归或神经网络时，回归算法本质上是在逼近复杂的函数关系。这时，回归模型可以看作是对真实关系的一个**近似**。
   
   - **优化中的回归**：
     有时，回归问题可以被视为一个优化问题。模型训练过程中，我们使用优化算法（如梯度下降）来最小化损失函数。对于一些复杂的回归问题，可能需要借助一些启发式的优化策略，从而获得接近最优的模型参数。

   - **回归和近似的共同特性**：
     二者都可能涉及“逼近”或“拟合”复杂的目标。例如，回归模型试图通过训练数据来拟合一个近似的函数，而近似算法通过计算找到复杂优化问题的接近解。因此，从广义上讲，回归算法在预测复杂数据关系时，可以被视为一种“函数近似”的方法。

### 5. **举例说明**
   - **回归问题的例子**：
     假设你想预测未来一段时间的股票价格。你收集了股票的历史价格、成交量、市场趋势等数据，然后使用线性回归、决策树或神经网络来建立一个模型，该模型拟合了输入特征与股票价格之间的关系。这个模型可以近似地表示股票价格的变化趋势，但它并不是一个精确的函数。

   - **近似算法的例子**：
     假设你需要解决一个大型组合优化问题，比如旅行商问题（TSP），寻找访问一组城市的最短路径。使用精确算法求解可能非常耗时，因此你使用遗传算法（近似算法）来快速得到一个接近最优的解。这是通过启发式策略实现的，但不会给出真正的最优解。

### 总结
- **回归算法**主要用于拟合和预测变量之间的关系，目标是预测连续值，是一种监督学习方法。
- **近似算法**用于复杂的优化或求解问题，通过启发式方法找到接近最优解的结果，属于优化领域。
- 虽然它们属于不同领域，但在一些场景中存在交叉，尤其是回归算法在建模复杂非线性关系时可以被看作是一种函数近似方法。


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE5ODE2MTY3ODYsLTIwODM0NDMyODIsLT
E3MjcyNTI1MjgsMTIzNTU5MjgxMiwtOTc4Mzk4MzQ4LDY2NTI5
OTM5LC0xMDMzOTc2MzA1LDExMTI1NzM2MTUsNzAzODYyMTE5LD
E4MzcxODEyOTYsNjc4MDQ2NDU2LC0yMDExOTI3NTUyLDkyMDY0
OTc2NCwtMjA3NjEwMzc4Myw4MjQ0Nzg2NTUsMTU5MzE2NDM2MC
wtODcxOTQwNDk0LC0xNTI2NDUzNDUzLDg1NjQ1NzA1MCw0NjE3
NDE4NV19
-->