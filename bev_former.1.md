

# bev_former

- 有6个编码层，主体框架与传统transformer一样
- 
## Transformer
QKV
q问的是在不在前面，k是回答，它俩相乘计算的是相关性，用相关性乘以v，得到一个delta变化的向量，基础向量加这个向量就会得到一个新的向量，这个向量会丰富原词语，在前面丰富，因为q问的是在不在前面，但是这个在不在前面是怎么体现的，应该是在回传更新的时候体现，或者是在不在前面这句话有一个向量encoding?那又是怎么体现的，又或者是在q*k后将后面的都设置为0？
q和k的维度会比较低，所以乘以的w维度与v的不同？q和k相当于作了降维，v没有降维，得到的就是一个与其他embedding在同等维度的向量，虽然是delta，但它要用于加法运算，也不能降维

“汇聚”通常指的是将多个向量或信息整合成一个单一的表示

k、v相当于已知的x_ i，y_i，q是未知的x，用已知的去求f(x)，
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTY2MDk3NDE3MiwtMTE5NjU5MzMyN119
-->